---
title: "Scale"
description: "Advanced patterns for scaling applications with ZK Compression"
---

## Overview

ZK Compression enables unprecedented scale on Solana. This guide covers advanced patterns and optimizations for building applications that can handle millions of users and transactions.

## Batch Operations

### Batch Token Transfers

Process multiple transfers in a single transaction:

```typescript
import { batchTransfer } from "@lightprotocol/zk-compression-sdk";

async function batchTokenTransfers(
  mint: PublicKey,
  transfers: Array<{ to: PublicKey; amount: number }>
) {
  const signature = await batchTransfer({
    connection,
    payer,
    mint,
    from: payer.publicKey,
    authority: payer,
    transfers: transfers.map((t) => ({
      destination: t.to,
      amount: t.amount * 10 ** 9,
    })),
  });

  console.log(`âœ… Batch transferred to ${transfers.length} recipients`);
  console.log(`Transaction: ${signature}`);

  return signature;
}

// Example usage
const recipients = [
  { to: new PublicKey("..."), amount: 100 },
  { to: new PublicKey("..."), amount: 200 },
  { to: new PublicKey("..."), amount: 150 },
];

await batchTokenTransfers(mint, recipients);
```

### Batch Minting

Mint tokens to multiple accounts efficiently:

```typescript
import { batchMintTo } from "@lightprotocol/zk-compression-sdk";

async function batchMint(
  mint: PublicKey,
  recipients: Array<{ account: PublicKey; amount: number }>
) {
  const signature = await batchMintTo({
    connection,
    payer,
    mint,
    authority: payer,
    destinations: recipients.map((r) => ({
      account: r.account,
      amount: r.amount * 10 ** 9,
    })),
  });

  console.log(`âœ… Batch minted to ${recipients.length} accounts`);
  return signature;
}
```

## Merkle Tree Optimization

### Tree Configuration

Optimize Merkle tree parameters for your use case:

```typescript
import { createMerkleTree } from "@lightprotocol/zk-compression-sdk";

async function createOptimizedTree(expectedAccounts: number) {
  // Calculate optimal tree depth based on expected accounts
  const depth = Math.ceil(Math.log2(expectedAccounts)) + 2; // Add buffer

  const tree = await createMerkleTree({
    connection,
    payer,
    maxDepth: Math.min(depth, 30), // Cap at 30 for performance
    maxBufferSize: 64, // Optimize for batch operations
    canopyDepth: 14, // Reduce proof size for common operations
  });

  console.log(`âœ… Created optimized tree with depth ${depth}`);
  return tree;
}
```

### Tree Management

Implement efficient tree management strategies:

```typescript
import {
  getTreeInfo,
  compactMerkleTree,
  rotateMerkleTree,
} from "@lightprotocol/zk-compression-sdk";

class TreeManager {
  private trees: PublicKey[] = [];
  private currentTreeIndex = 0;

  async initializeTrees(count: number) {
    for (let i = 0; i < count; i++) {
      const tree = await createOptimizedTree(1000000); // 1M accounts per tree
      this.trees.push(tree);
    }
    console.log(`âœ… Initialized ${count} trees`);
  }

  async getCurrentTree(): Promise<PublicKey> {
    const tree = this.trees[this.currentTreeIndex];
    const info = await getTreeInfo({ connection, tree });

    // Rotate to next tree if current is getting full
    if (info.accountCount > info.maxAccounts * 0.8) {
      this.currentTreeIndex = (this.currentTreeIndex + 1) % this.trees.length;
      console.log(`ðŸ”„ Rotated to tree ${this.currentTreeIndex}`);
    }

    return this.trees[this.currentTreeIndex];
  }

  async compactTrees() {
    for (const tree of this.trees) {
      await compactMerkleTree({ connection, payer, tree });
    }
    console.log("âœ… Compacted all trees");
  }
}
```

## Parallel Processing

### Concurrent Operations

Process operations in parallel for maximum throughput:

```typescript
import { Worker } from "worker_threads";

class ParallelProcessor {
  private workers: Worker[] = [];
  private taskQueue: any[] = [];

  constructor(workerCount: number = 4) {
    for (let i = 0; i < workerCount; i++) {
      const worker = new Worker("./compression-worker.js");
      this.workers.push(worker);
    }
  }

  async processTransfers(transfers: any[]) {
    const chunks = this.chunkArray(transfers, this.workers.length);

    const promises = chunks.map((chunk, index) =>
      this.processChunk(chunk, index)
    );

    const results = await Promise.all(promises);
    return results.flat();
  }

  private async processChunk(chunk: any[], workerIndex: number) {
    return new Promise((resolve, reject) => {
      const worker = this.workers[workerIndex];

      worker.postMessage({ type: "PROCESS_TRANSFERS", data: chunk });

      worker.once("message", (result) => {
        if (result.success) {
          resolve(result.data);
        } else {
          reject(new Error(result.error));
        }
      });
    });
  }

  private chunkArray<T>(array: T[], chunkSize: number): T[][] {
    const chunks: T[][] = [];
    for (let i = 0; i < array.length; i += chunkSize) {
      chunks.push(array.slice(i, i + chunkSize));
    }
    return chunks;
  }
}
```

### Rate Limiting

Implement intelligent rate limiting to avoid RPC limits:

```typescript
class RateLimiter {
  private requests: number[] = [];
  private maxRequestsPerSecond: number;

  constructor(maxRequestsPerSecond: number = 100) {
    this.maxRequestsPerSecond = maxRequestsPerSecond;
  }

  async throttle(): Promise<void> {
    const now = Date.now();

    // Remove requests older than 1 second
    this.requests = this.requests.filter((time) => now - time < 1000);

    if (this.requests.length >= this.maxRequestsPerSecond) {
      const oldestRequest = Math.min(...this.requests);
      const waitTime = 1000 - (now - oldestRequest);

      if (waitTime > 0) {
        await new Promise((resolve) => setTimeout(resolve, waitTime));
      }
    }

    this.requests.push(now);
  }
}

// Usage with batch operations
const rateLimiter = new RateLimiter(50); // 50 requests per second

async function scaledBatchTransfer(transfers: any[]) {
  const batches = chunkArray(transfers, 10); // 10 transfers per batch

  for (const batch of batches) {
    await rateLimiter.throttle();
    await batchTransfer(batch);
  }
}
```

## Caching Strategies

### State Caching

Implement efficient caching for frequently accessed state:

```typescript
import { LRUCache } from "lru-cache";

class StateCache {
  private accountCache = new LRUCache<string, any>({
    max: 10000,
    ttl: 1000 * 60 * 5, // 5 minutes
  });

  private balanceCache = new LRUCache<string, number>({
    max: 50000,
    ttl: 1000 * 30, // 30 seconds
  });

  async getCompressedAccount(accountHash: string) {
    const cached = this.accountCache.get(accountHash);
    if (cached) {
      return cached;
    }

    const account = await getCompressedAccount({
      connection,
      accountHash,
    });

    this.accountCache.set(accountHash, account);
    return account;
  }

  async getBalance(mint: PublicKey, owner: PublicKey) {
    const key = `${mint.toBase58()}-${owner.toBase58()}`;
    const cached = this.balanceCache.get(key);

    if (cached !== undefined) {
      return cached;
    }

    const balance = await getCompressedTokenBalance({
      connection,
      mint,
      owner,
    });

    this.balanceCache.set(key, balance);
    return balance;
  }

  invalidateAccount(accountHash: string) {
    this.accountCache.delete(accountHash);
  }

  invalidateBalance(mint: PublicKey, owner: PublicKey) {
    const key = `${mint.toBase58()}-${owner.toBase58()}`;
    this.balanceCache.delete(key);
  }
}
```

### Preloading

Preload frequently accessed data:

```typescript
class DataPreloader {
  private cache: StateCache;

  constructor(cache: StateCache) {
    this.cache = cache;
  }

  async preloadUserData(userPublicKey: PublicKey) {
    // Preload all user's compressed token accounts
    const accounts = await getCompressedTokenAccountsByOwner({
      connection,
      owner: userPublicKey,
    });

    // Preload balances for all mints
    const balancePromises = accounts.map((account) =>
      this.cache.getBalance(account.mint, userPublicKey)
    );

    await Promise.all(balancePromises);
    console.log(`âœ… Preloaded data for ${accounts.length} accounts`);
  }

  async preloadPopularMints(mints: PublicKey[]) {
    // Preload metadata and stats for popular mints
    const promises = mints.map(async (mint) => {
      const [metadata, stats] = await Promise.all([
        getMintMetadata({ connection, mint }),
        getMintStats({ connection, mint }),
      ]);

      return { mint, metadata, stats };
    });

    const results = await Promise.all(promises);
    console.log(`âœ… Preloaded data for ${results.length} mints`);

    return results;
  }
}
```

## Monitoring and Analytics

### Performance Monitoring

Track performance metrics for optimization:

```typescript
class PerformanceMonitor {
  private metrics: Map<string, number[]> = new Map();

  startTimer(operation: string): () => void {
    const start = performance.now();

    return () => {
      const duration = performance.now() - start;
      this.recordMetric(operation, duration);
    };
  }

  recordMetric(operation: string, value: number) {
    if (!this.metrics.has(operation)) {
      this.metrics.set(operation, []);
    }

    const values = this.metrics.get(operation)!;
    values.push(value);

    // Keep only last 1000 measurements
    if (values.length > 1000) {
      values.shift();
    }
  }

  getStats(operation: string) {
    const values = this.metrics.get(operation) || [];
    if (values.length === 0) return null;

    const sorted = [...values].sort((a, b) => a - b);
    const avg = values.reduce((a, b) => a + b, 0) / values.length;
    const p50 = sorted[Math.floor(sorted.length * 0.5)];
    const p95 = sorted[Math.floor(sorted.length * 0.95)];
    const p99 = sorted[Math.floor(sorted.length * 0.99)];

    return { avg, p50, p95, p99, count: values.length };
  }

  logStats() {
    console.log("\nðŸ“Š Performance Stats:");
    for (const [operation, _] of this.metrics) {
      const stats = this.getStats(operation);
      if (stats) {
        console.log(`${operation}:`);
        console.log(`  Average: ${stats.avg.toFixed(2)}ms`);
        console.log(`  P95: ${stats.p95.toFixed(2)}ms`);
        console.log(`  Count: ${stats.count}`);
      }
    }
  }
}

// Usage
const monitor = new PerformanceMonitor();

async function monitoredTransfer(
  mint: PublicKey,
  to: PublicKey,
  amount: number
) {
  const endTimer = monitor.startTimer("transfer");

  try {
    const result = await transfer({
      connection,
      payer,
      mint,
      from: payer.publicKey,
      to,
      authority: payer,
      amount,
    });

    return result;
  } finally {
    endTimer();
  }
}
```

## Best Practices for Scale

### 1. Design for Horizontal Scaling

- Use multiple Merkle trees to distribute load
- Implement sharding strategies for large datasets
- Design stateless operations where possible

### 2. Optimize Network Usage

- Batch operations whenever possible
- Use compression for data transfer
- Implement intelligent retry mechanisms

### 3. Monitor and Alert

- Track key performance metrics
- Set up alerts for degraded performance
- Monitor RPC usage and costs

### 4. Plan for Growth

- Design flexible tree structures
- Implement data archiving strategies
- Plan for infrastructure scaling

## Next Steps

<CardGroup cols={2}>
  <Card title="Token Metadata" icon="tags" href="/next-steps/token-metadata">
    Add rich metadata to your compressed tokens
  </Card>
  <Card title="API Reference" icon="code" href="/api-reference/json-rpc">
    Explore the complete API documentation
  </Card>
</CardGroup>{" "}
